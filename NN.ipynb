{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLR model construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data\n",
    "data = pd.read_csv('ps_usable_hydrogen_storage_capacity_gcmcv2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information about the data\n",
    "This dataset contains 98,694 different MOFs with 7 structural properties and usable gravimetric storage capacity (UG) and usable volumetric storage capacity (UV) to measure the hydrogen storage capacity of each MOF. \n",
    "\n",
    "7 Structural properties,  2 responses and their units of this dataset are provided as follows:\n",
    "\n",
    "Property | unit\n",
    "--- | ---\n",
    "Density | $$g/cm^3$$\n",
    "Gravimetric surface area (GSA) | $$m^2/g$$\n",
    "Volumetric surface area (VSA) | $$m^2/cm^3$$\n",
    "Void fracion (VF)| \n",
    "Pore volumn (PV)| $$cm^3/g$$\n",
    "Largest cavity diameter (LCD) | $$Å$$\n",
    "Pore limiting diameter (PLD) | $$Å$$\n",
    "Usable gravimetric storage capacity (UG) | $$wt.\\%$$\n",
    "Usable volumetric storage capacity (UV) | $$g/cm^3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the size of the loaded data \n",
    "assert(data.shape[0] == 98694)\n",
    "assert(data.shape[1] == 17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Column name modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns=lambda x: x.rstrip()) # delete the extra space in the end and check it again\n",
    "data = data.rename(columns={'UG at PS':'UG', 'UV at PS': 'UV'}) # simplify some columns' names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Abnormal data removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove data whose features are smaller or equal to 0\n",
    "features_name = data.columns[5:12].tolist()\n",
    "for feature in features_name:\n",
    "    data = data.drop(data[data[feature] <= 0].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Nan of Density: 0\n",
      "  Nan of GSA: 0\n",
      "  Nan of VSA: 0\n",
      "  Nan of VF: 0\n",
      "  Nan of PV: 0\n",
      "  Nan of LCD: 0\n",
      "  Nan of PLD: 0\n"
     ]
    }
   ],
   "source": [
    "def check_nan(col, data):\n",
    "    '''Count the number of nan for a specific column in a dataset.'''\n",
    "    return data.shape[0] - data[col].dropna().size\n",
    "\n",
    "def print_nan(features):\n",
    "    '''Print nan for original_training_data'''\n",
    "    for feature in features:\n",
    "        print('  Nan of ' + str(feature) + ': ' + str(check_nan(feature, data)))\n",
    "\n",
    "print_nan(features_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the cleaned dataset and the original one, one can found that there are some abnormal data in this dataset. The structures with features that are smaller or equal to 0 can be due to some possible mistakes when collecting the data and are impossible to shown outstanding hydrogen adsorption performance according to the discovered relationships between these features and adsorption capacity. However, there is no missing value for the 7 features and the 2 responses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visulization of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions of single features and UG/UV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dist(data, var_names):\n",
    "    '''Plot the distribution of each variable.'''\n",
    "    for var in var_names:\n",
    "        plt.figure(figsize=(20, 6))\n",
    "        sns.histplot(data[var], kde = True)\n",
    "        plt.xlabel(var, fontsize = 15)\n",
    "        plt.ylabel('Count', fontsize = 15)\n",
    "        \n",
    "features_name.append('UG')\n",
    "features_name.append('UV')\n",
    "#plot_dist(data, features_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above visulizations, one can observe that the distributions of different features and responses are extremely different. Some of them are right skewed while others are left skewed. Also, they have different ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship between UV and UG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.761160782050035"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = np.corrcoef(data['UV'],data['UG'])[0][1]\n",
    "r1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above figure, one can see that the linear correlation between the two reponses is not as high as expected. Points with relatively high UG and low UV occur in the dataset. Intuitively, this is related to the low density of these structures. The below visulization proves this assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship between each structural property and UG/UV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_feature_plot(data, single_feature):\n",
    "    '''Plot the relationship between single feature and UG/UV'''\n",
    "    \n",
    "    # check that the input single_feature has type of str\n",
    "    try:\n",
    "        assert(type(single_feature) == str)\n",
    "    except:\n",
    "        raise TypeError('The input single_feature is not string.')\n",
    "\n",
    "    if single_feature == 'Density':\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (15, 6))\n",
    "        ax1.scatter(data[single_feature], data['UG'])\n",
    "        ax1.set_xlabel(single_feature, fontsize = 15)\n",
    "        ax1.set_ylabel('UG',fontsize = 15)\n",
    "        ax1.set_title('Relationship between '+single_feature+ ' and UG', fontsize = 15)\n",
    "        ax2.scatter(data[single_feature], data['UV'])\n",
    "        ax2.set_xlabel(single_feature, fontsize = 15)\n",
    "        ax2.set_ylabel('UV', fontsize = 15)\n",
    "        ax2.set_title('Relationship between '+single_feature+ ' and UV', fontsize = 15)\n",
    "    else:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (15, 5))\n",
    "        sc1 = ax1.scatter(data[single_feature], data['UG'], c = data['Density'], cmap = cm)\n",
    "        ax1.set_xlabel(single_feature, fontsize = 15)\n",
    "        ax1.set_ylabel('UG',fontsize = 15)\n",
    "        ax1.set_title('Relationship between '+single_feature+ ' and UG', fontsize = 15)\n",
    "        fig.colorbar(sc1, ax = ax1)\n",
    "        sc2 = ax2.scatter(data[single_feature], data['UV'], c = data['Density'], cmap = cm)\n",
    "        ax2.set_xlabel(single_feature, fontsize = 15)\n",
    "        ax2.set_ylabel('UV', fontsize = 15)\n",
    "        ax2.set_title('Relationship between '+single_feature+ ' and UV', fontsize = 15)\n",
    "        fig.colorbar(sc2, ax = ax2)\n",
    "\n",
    "#for single_feature in features_name[:7]:\n",
    "#    single_feature_plot(data, single_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify the importance of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UV</th>\n",
       "      <th>UG</th>\n",
       "      <th>Avg_corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Density</th>\n",
       "      <td>-0.859508</td>\n",
       "      <td>-0.766090</td>\n",
       "      <td>-0.812799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSA</th>\n",
       "      <td>0.879863</td>\n",
       "      <td>0.832255</td>\n",
       "      <td>0.856059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VSA</th>\n",
       "      <td>0.532700</td>\n",
       "      <td>0.083976</td>\n",
       "      <td>0.308338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VF</th>\n",
       "      <td>0.931162</td>\n",
       "      <td>0.792888</td>\n",
       "      <td>0.862025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PV</th>\n",
       "      <td>0.531987</td>\n",
       "      <td>0.933700</td>\n",
       "      <td>0.732844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LCD</th>\n",
       "      <td>0.598503</td>\n",
       "      <td>0.828389</td>\n",
       "      <td>0.713446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLD</th>\n",
       "      <td>0.616799</td>\n",
       "      <td>0.826018</td>\n",
       "      <td>0.721409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               UV        UG  Avg_corr\n",
       "Density -0.859508 -0.766090 -0.812799\n",
       "GSA      0.879863  0.832255  0.856059\n",
       "VSA      0.532700  0.083976  0.308338\n",
       "VF       0.931162  0.792888  0.862025\n",
       "PV       0.531987  0.933700  0.732844\n",
       "LCD      0.598503  0.828389  0.713446\n",
       "PLD      0.616799  0.826018  0.721409"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating the correlation coefficient\n",
    "corr_coefs = []\n",
    "for single_feature in features_name[:7]:\n",
    "    corr_coefs.append([np.corrcoef(data[single_feature],data['UV'])[0][1],np.corrcoef(data[single_feature],data['UG'])[0][1]])\n",
    "\n",
    "df_rs = pd.DataFrame(columns =['UV', 'UG'], data = corr_coefs, index = features_name[:7])\n",
    "df_rs['Avg_corr'] = (df_rs['UV'] + df_rs['UG'])/2\n",
    "df_rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the above analysis, density is negatively correlated with UG/UV while other features are positively correlated with UG/UV. However, the linear correlation between the features (except density) and responses are not strong. This can be due to the differences of density. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features ranking:\n",
      "   UV + UG: \n",
      "   ['VF', 'GSA', 'Density', 'PV', 'PLD', 'LCD', 'VSA']\n",
      "   UV: \n",
      "   ['VF', 'GSA', 'Density', 'PLD', 'LCD', 'VSA', 'PV']\n",
      "   UG: \n",
      "   ['PV', 'GSA', 'LCD', 'PLD', 'VF', 'Density', 'VSA']\n"
     ]
    }
   ],
   "source": [
    "df_rs_abs = df_rs.copy()\n",
    "df_rs_abs = abs(df_rs_abs) # consider the feature importance as its absolute value\n",
    "df_rs_abs_1 = df_rs_abs.sort_values(by = 'Avg_corr', ascending = False)\n",
    "df_rs_abs_2 = df_rs_abs.sort_values(by = 'UV', ascending = False)\n",
    "df_rs_abs_3 = df_rs_abs.sort_values(by = 'UG', ascending = False)\n",
    "print('Features ranking:')\n",
    "print('   UV + UG: ')\n",
    "print('   '+str(df_rs_abs_1.index.tolist()))\n",
    "print('   UV: ')\n",
    "print('   '+str(df_rs_abs_2.index.tolist()))\n",
    "print('   UG: ')\n",
    "print('   '+str(df_rs_abs_3.index.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above analysis, we can found that the VF has the biggest influence on the overall hydrogen adsorption performance while the VSA has the smallest influence on the overall hydrogen adsorption performance. In terms of UV, the most important factor is VF and the least important is VSA. For UG, the most important factor is PV and the least important one is VSA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data transformation\n",
    "Data transformation can be hard because it requires to achieve linear relationships between each features and two responses simultaneously. Herein, I try my best to do the data transformation to achieve this by tring both logrithm transformation and power transformation of different values. Only the best results were left in this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty list to store the transformed average correlation coefficients\n",
    "transformed_avg_corrs = []\n",
    "\n",
    "# Density\n",
    "data_trans = data.copy()\n",
    "data_trans['Density'] = -np.log(data_trans['Density'])\n",
    "val1 = sum([np.corrcoef(data_trans['Density'],data['UV'])[0][1],np.corrcoef(data_trans['Density'],data['UG'])[0][1]])/2\n",
    "transformed_avg_corrs.append(val1)\n",
    "\n",
    "# GSA\n",
    "data_trans['GSA'] = data_trans['GSA']**1.1\n",
    "val2 = sum([np.corrcoef(data_trans['GSA'],data['UV'])[0][1],np.corrcoef(data_trans['GSA'],data['UG'])[0][1]])/2\n",
    "transformed_avg_corrs.append(val2)\n",
    "\n",
    "# VSA\n",
    "data_trans['VSA'] = (data_trans['VSA'])**0.2\n",
    "val3 = sum([np.corrcoef(data_trans['VSA'],data['UV'])[0][1],np.corrcoef(data_trans['VSA'],data['UG'])[0][1]])/2\n",
    "transformed_avg_corrs.append(val3)\n",
    "\n",
    "# VF\n",
    "data_trans['VF'] = (data_trans['VF'])**4.2\n",
    "val4 = sum([np.corrcoef(data_trans['VF'],data['UV'])[0][1],np.corrcoef(data_trans['VF'],data['UG'])[0][1]])/2\n",
    "transformed_avg_corrs.append(val4)\n",
    "\n",
    "# PV\n",
    "data_trans['PV'] = (data_trans['PV'])**0.1\n",
    "val5 = sum([np.corrcoef(data_trans['PV'],data['UV'])[0][1],np.corrcoef(data_trans['PV'],data['UG'])[0][1]])/2\n",
    "transformed_avg_corrs.append(val5)\n",
    "\n",
    "# LCD\n",
    "data_trans['LCD'] = np.log(data_trans['LCD'])\n",
    "val6 = sum([np.corrcoef(data_trans['LCD'],data['UV'])[0][1],np.corrcoef(data_trans['LCD'],data['UG'])[0][1]])/2\n",
    "transformed_avg_corrs.append(val6)\n",
    "\n",
    "# PLD\n",
    "data_trans['PLD'] = np.log(data_trans['PLD'])\n",
    "val7 = sum([np.corrcoef(data_trans['PLD'],data['UV'])[0][1],np.corrcoef(data_trans['PLD'],data['UG'])[0][1]])/2\n",
    "transformed_avg_corrs.append(val7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rs['Transformed_avg_corr'] = transformed_avg_corrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform all the features to have better linear relationship to the 2 responses may lead to higher accuracy of the multi-linear regression model. But whether this assumption is true still requires further modeling work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_name = ['Density', 'GSA', 'VSA', 'VF', 'PV', 'LCD', 'PLD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single output model\n",
    "Step 1. trans or not -- it doesn't matter -- no trans\n",
    "\n",
    "Step 2. batch_size -- it doesn't matter -- batch_size = len(trains)\n",
    "\n",
    "Step 3. which act_func and number of dense layer and epoch\n",
    "* Best act_func is sigmoid\n",
    "* Number of dense layer doesn't help to increase the accuracy measured by mse.\n",
    "* The larger the number of epoch, the better the model. But when epoch > 7, the influcence is insignificant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1. trans or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, train_size = 0.9, random_state=42)\n",
    "train_trans, test_trans = train_test_split(data_trans, train_size = 0.9, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_model_single(inputs, num, act, res):\n",
    "    inputs = tf.keras.Input(shape=(7))\n",
    "    x = tf.keras.layers.Dense(7, activation = act)(inputs)\n",
    "    if num >= 1:\n",
    "        for _ in range(num-1):\n",
    "            x = tf.keras.layers.Dense(7, activation = act)(x)\n",
    "            \n",
    "    outputs = tf.keras.layers.Dense(1, name = res, activation = act)(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_and_compile_model(num, act, res, lr=0.1):\n",
    "    '''define and compile the model\n",
    "    num: int -- number of dense layer in the model - 1\n",
    "    act: str -- activation function\n",
    "    '''\n",
    "    inputs = tf.keras.Input(shape = (7))\n",
    "    model = final_model_single(inputs, num, act, res)\n",
    "    \n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "    model.compile(optimizer = opt, loss = 'mse', metrics = 'mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(train, test, num, act, epoch, batch, res, lr):\n",
    "    model = define_and_compile_model(num, act, res, lr)\n",
    "    model.summary()\n",
    "    history = model.fit(train[features_name], train[res], epochs = epoch, batch_size = batch, validation_data = (test[features_name], test[res]))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UV (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4852594.5000 - mse: 4852594.5000 - val_loss: 679.8645 - val_mse: 679.8645\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 685.2281 - mse: 685.2281 - val_loss: 679.8645 - val_mse: 679.8645\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 685.2281 - mse: 685.2281 - val_loss: 679.8645 - val_mse: 679.8645\n"
     ]
    }
   ],
   "source": [
    "mdoel_UV_trans = model_pipeline(train_trans, test_trans, 0, 'relu', 3, len(train_trans), 'UV', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UV (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "1/1 [==============================] - 0s 494ms/step - loss: 235336.7500 - mse: 235336.7500 - val_loss: 679.8645 - val_mse: 679.8645\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 685.2281 - mse: 685.2281 - val_loss: 679.8645 - val_mse: 679.8645\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 685.2281 - mse: 685.2281 - val_loss: 679.8645 - val_mse: 679.8645\n"
     ]
    }
   ],
   "source": [
    "mdoel_UV = model_pipeline(train, test, 0, 'relu', 3, len(train), 'UV', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "1/1 [==============================] - 0s 470ms/step - loss: 32.1102 - mse: 32.1102 - val_loss: 31.6052 - val_mse: 31.6052\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 32.1102 - mse: 32.1102 - val_loss: 31.6052 - val_mse: 31.6052\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 32.1102 - mse: 32.1102 - val_loss: 31.6052 - val_mse: 31.6052\n"
     ]
    }
   ],
   "source": [
    "mdoel_UG_trans = model_pipeline(train_trans, test_trans, 0, 'relu', 3, len(train_trans), 'UG', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "1/1 [==============================] - 0s 473ms/step - loss: 4841283.0000 - mse: 4841283.0000 - val_loss: 31.6052 - val_mse: 31.6052\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 32.1102 - mse: 32.1102 - val_loss: 31.6052 - val_mse: 31.6052\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 32.1102 - mse: 32.1102 - val_loss: 31.6052 - val_mse: 31.6052\n"
     ]
    }
   ],
   "source": [
    "mdoel_UG_trans = model_pipeline(train, test, 0, 'relu', 3, len(train), 'UG', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff8232b64d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "793/797 [============================>.] - ETA: 0s - loss: 1452.4231 - mse: 1452.4231WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff8247d6560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "797/797 [==============================] - 1s 1ms/step - loss: 1444.7465 - mse: 1444.7465 - val_loss: 31.6052 - val_mse: 31.6052\n",
      "Epoch 2/3\n",
      "797/797 [==============================] - 1s 1ms/step - loss: 32.3137 - mse: 32.3137 - val_loss: 31.6052 - val_mse: 31.6052\n",
      "Epoch 3/3\n",
      "797/797 [==============================] - 1s 1ms/step - loss: 31.9553 - mse: 31.9553 - val_loss: 31.6052 - val_mse: 31.6052\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "80/80 [==============================] - 1s 4ms/step - loss: 91.5137 - mse: 91.5137 - val_loss: 31.6052 - val_mse: 31.6052\n",
      "Epoch 2/3\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 32.1030 - mse: 32.1030 - val_loss: 31.6052 - val_mse: 31.6052\n",
      "Epoch 3/3\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 32.2166 - mse: 32.2166 - val_loss: 31.6052 - val_mse: 31.6052\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "8/8 [==============================] - 1s 30ms/step - loss: 32.1960 - mse: 32.1960 - val_loss: 31.6052 - val_mse: 31.6052\n",
      "Epoch 2/3\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 31.8753 - mse: 31.8753 - val_loss: 31.6052 - val_mse: 31.6052\n",
      "Epoch 3/3\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 31.7402 - mse: 31.7402 - val_loss: 31.6052 - val_mse: 31.6052\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "1/1 [==============================] - 0s 472ms/step - loss: 238188.6094 - mse: 238188.6094 - val_loss: 31.6052 - val_mse: 31.6052\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 32.1102 - mse: 32.1102 - val_loss: 31.6052 - val_mse: 31.6052\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 32.1102 - mse: 32.1102 - val_loss: 31.6052 - val_mse: 31.6052\n"
     ]
    }
   ],
   "source": [
    "batches = [100, 1000, 10000, len(train)]\n",
    "for batch in batches:\n",
    "    model_pipeline(train, test, 0, 'relu', 3, batch,'UG', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_18 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UV (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "797/797 [==============================] - 1s 1ms/step - loss: 8304.5958 - mse: 8304.5958 - val_loss: 679.8645 - val_mse: 679.8645\n",
      "Epoch 2/3\n",
      "797/797 [==============================] - 1s 1ms/step - loss: 684.8846 - mse: 684.8846 - val_loss: 679.8645 - val_mse: 679.8645\n",
      "Epoch 3/3\n",
      "797/797 [==============================] - 1s 1ms/step - loss: 684.5460 - mse: 684.5460 - val_loss: 679.8645 - val_mse: 679.8645\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UV (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "80/80 [==============================] - 1s 4ms/step - loss: 788.0045 - mse: 788.0045 - val_loss: 679.8644 - val_mse: 679.8644\n",
      "Epoch 2/3\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 687.7207 - mse: 687.7207 - val_loss: 679.8644 - val_mse: 679.8644\n",
      "Epoch 3/3\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 682.8393 - mse: 682.8393 - val_loss: 679.8644 - val_mse: 679.8644\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_22 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UV (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "8/8 [==============================] - 1s 30ms/step - loss: 684.9018 - mse: 684.9018 - val_loss: 679.8645 - val_mse: 679.8645\n",
      "Epoch 2/3\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 685.0600 - mse: 685.0600 - val_loss: 679.8645 - val_mse: 679.8645\n",
      "Epoch 3/3\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 685.3301 - mse: 685.3301 - val_loss: 679.8645 - val_mse: 679.8645\n",
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_24 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UV (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "1/1 [==============================] - 0s 464ms/step - loss: 2396775.2500 - mse: 2396775.2500 - val_loss: 679.8645 - val_mse: 679.8645\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 685.2281 - mse: 685.2281 - val_loss: 679.8645 - val_mse: 679.8645\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 685.2281 - mse: 685.2281 - val_loss: 679.8645 - val_mse: 679.8645\n"
     ]
    }
   ],
   "source": [
    "for batch in batches:\n",
    "    model_pipeline(train, test, 0, 'relu', 3, batch,'UV', 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: batch size doesn't help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation func is relu\n",
      "Number of dense layer is 1\n",
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_26 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UV (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "1/1 [==============================] - 0s 484ms/step - loss: 1403.6920 - mse: 1403.6920 - val_loss: 679.8645 - val_mse: 679.8645\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 685.2281 - mse: 685.2281 - val_loss: 679.8645 - val_mse: 679.8645\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 685.2281 - mse: 685.2281 - val_loss: 679.8645 - val_mse: 679.8645\n",
      "Activation func is relu\n",
      "Number of dense layer is 2\n",
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_28 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UV (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 120\n",
      "Trainable params: 120\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "1/1 [==============================] - ETA: 0s - loss: 724893.3750 - mse: 724893.3750WARNING:tensorflow:5 out of the last 37 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff8249d7e60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 711ms/step - loss: 724893.3750 - mse: 724893.3750 - val_loss: 679.8645 - val_mse: 679.8645\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 685.2281 - mse: 685.2281 - val_loss: 679.8645 - val_mse: 679.8645\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 685.2281 - mse: 685.2281 - val_loss: 679.8645 - val_mse: 679.8645\n",
      "Activation func is relu\n",
      "Number of dense layer is 3\n",
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_30 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UV (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 176\n",
      "Trainable params: 176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 34 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff826c3d050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 685.2281 - mse: 685.2281WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff823d9c3b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 778ms/step - loss: 685.2281 - mse: 685.2281 - val_loss: 679.8645 - val_mse: 679.8645\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 685.2281 - mse: 685.2281 - val_loss: 679.8645 - val_mse: 679.8645\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 685.2281 - mse: 685.2281 - val_loss: 679.8645 - val_mse: 679.8645\n",
      "Activation func is sigmoid\n",
      "Number of dense layer is 1\n",
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_32 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UV (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff8244bf290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 677.8876 - mse: 677.8876WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff824acb170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 684ms/step - loss: 677.8876 - mse: 677.8876 - val_loss: 645.7389 - val_mse: 645.7389\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 650.9371 - mse: 650.9371 - val_loss: 632.6557 - val_mse: 632.6557\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 637.7997 - mse: 637.7997 - val_loss: 632.6282 - val_mse: 632.6282\n",
      "Activation func is sigmoid\n",
      "Number of dense layer is 2\n",
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_34 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UV (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 120\n",
      "Trainable params: 120\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff824adb830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 643.3400 - mse: 643.3400WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff823bb6d40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 622ms/step - loss: 643.3400 - mse: 643.3400 - val_loss: 633.3813 - val_mse: 633.3813\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 638.5273 - mse: 638.5273 - val_loss: 633.0094 - val_mse: 633.0094\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 638.1545 - mse: 638.1545 - val_loss: 632.8676 - val_mse: 632.8676\n",
      "Activation func is sigmoid\n",
      "Number of dense layer is 3\n",
      "Model: \"model_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_36 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UV (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 176\n",
      "Trainable params: 176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff827e6ce60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 657.8463 - mse: 657.8463WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff822a7f290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 577ms/step - loss: 657.8463 - mse: 657.8463 - val_loss: 633.0900 - val_mse: 633.0900\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 638.2357 - mse: 638.2357 - val_loss: 632.9312 - val_mse: 632.9312\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 638.0761 - mse: 638.0761 - val_loss: 632.8323 - val_mse: 632.8323\n",
      "Activation func is softmax\n",
      "Number of dense layer is 1\n",
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_38 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UV (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff8228a8170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 637.5560 - mse: 637.5560WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff820e6f7a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 801ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Activation func is softmax\n",
      "Number of dense layer is 2\n",
      "Model: \"model_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_40 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UV (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 120\n",
      "Trainable params: 120\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff82abb0710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 637.5560 - mse: 637.5560WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff8265c9c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 825ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Activation func is softmax\n",
      "Number of dense layer is 3\n",
      "Model: \"model_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_42 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UV (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 176\n",
      "Trainable params: 176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff82ad087a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 637.5560 - mse: 637.5560WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff82f317050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 898ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n"
     ]
    }
   ],
   "source": [
    "act_funcs = ['relu', 'sigmoid','softmax']\n",
    "nums = [1, 2, 3]\n",
    "for act in act_funcs:\n",
    "    for num in nums:\n",
    "        print(\"Activation func is \"+act)\n",
    "        print(\"Number of dense layer is \"+str(num))\n",
    "        model_pipeline(train, test, num, act, 3, len(train), 'UV', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation func is relu\n",
      "Number of dense layer is 1\n",
      "Model: \"model_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_44 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff81d825cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 743.8095 - mse: 743.8095WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff8232a69e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 475ms/step - loss: 743.8095 - mse: 743.8095 - val_loss: 31.6052 - val_mse: 31.6052\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 32.1102 - mse: 32.1102 - val_loss: 31.6052 - val_mse: 31.6052\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 32.1102 - mse: 32.1102 - val_loss: 31.6052 - val_mse: 31.6052\n",
      "Activation func is relu\n",
      "Number of dense layer is 2\n",
      "Model: \"model_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_46 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 120\n",
      "Trainable params: 120\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff81bc394d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 545348.1250 - mse: 545348.1250WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff8232a68c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 652ms/step - loss: 545348.1250 - mse: 545348.1250 - val_loss: 31.6052 - val_mse: 31.6052\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 32.1102 - mse: 32.1102 - val_loss: 31.6052 - val_mse: 31.6052\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 32.1102 - mse: 32.1102 - val_loss: 31.6052 - val_mse: 31.6052\n",
      "Activation func is relu\n",
      "Number of dense layer is 3\n",
      "Model: \"model_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_48 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 176\n",
      "Trainable params: 176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff82f3175f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 82.3512 - mse: 82.3512WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff8232a6e60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 564ms/step - loss: 82.3512 - mse: 82.3512 - val_loss: 31.6052 - val_mse: 31.6052\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 32.1102 - mse: 32.1102 - val_loss: 31.6052 - val_mse: 31.6052\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 32.1102 - mse: 32.1102 - val_loss: 31.6052 - val_mse: 31.6052\n",
      "Activation func is sigmoid\n",
      "Number of dense layer is 1\n",
      "Model: \"model_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_50 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff82abb0cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 28.4169 - mse: 28.4169WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff8232a6b90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 475ms/step - loss: 28.4169 - mse: 28.4169 - val_loss: 26.5008 - val_mse: 26.5008\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 26.9496 - mse: 26.9496 - val_loss: 25.4764 - val_mse: 25.4764\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 25.9141 - mse: 25.9141 - val_loss: 24.9134 - val_mse: 24.9134\n",
      "Activation func is sigmoid\n",
      "Number of dense layer is 2\n",
      "Model: \"model_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_52 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 120\n",
      "Trainable params: 120\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff823d6be60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 27.6518 - mse: 27.6518WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff8232a6dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 498ms/step - loss: 27.6518 - mse: 27.6518 - val_loss: 25.9926 - val_mse: 25.9926\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 26.4364 - mse: 26.4364 - val_loss: 25.1726 - val_mse: 25.1726\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 25.6043 - mse: 25.6043 - val_loss: 24.7432 - val_mse: 24.7432\n",
      "Activation func is sigmoid\n",
      "Number of dense layer is 3\n",
      "Model: \"model_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_54 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 176\n",
      "Trainable params: 176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff8232a6b90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 28.6003 - mse: 28.6003WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff8265c93b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 538ms/step - loss: 28.6003 - mse: 28.6003 - val_loss: 26.9937 - val_mse: 26.9937\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 27.4483 - mse: 27.4483 - val_loss: 25.9952 - val_mse: 25.9952\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 26.4380 - mse: 26.4380 - val_loss: 25.3239 - val_mse: 25.3239\n",
      "Activation func is softmax\n",
      "Number of dense layer is 1\n",
      "Model: \"model_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_56 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff8265c9e60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 24.2310 - mse: 24.2310WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff820e6f7a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 685ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Activation func is softmax\n",
      "Number of dense layer is 2\n",
      "Model: \"model_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_58 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 120\n",
      "Trainable params: 120\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff822a7f680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 24.2310 - mse: 24.2310WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff821717320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 577ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Activation func is softmax\n",
      "Number of dense layer is 3\n",
      "Model: \"model_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_60 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 176\n",
      "Trainable params: 176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff821717c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 24.2310 - mse: 24.2310WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff826bb45f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 754ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n"
     ]
    }
   ],
   "source": [
    "for act in act_funcs:\n",
    "    for num in nums:\n",
    "        print(\"Activation func is \"+act)\n",
    "        print(\"Number of dense layer is \"+str(num))\n",
    "        model_pipeline(train, test, num, act, 3, len(train), 'UG', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate is 0.01\n",
      "Model: \"model_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_62 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UV (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff81ddf7830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 637.5560 - mse: 637.5560WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff8211eccb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 504ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "learning rate is 0.1\n",
      "Model: \"model_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_64 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UV (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff8247eecb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 637.5560 - mse: 637.5560WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff823132b90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 508ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "learning rate is 0.5\n",
      "Model: \"model_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_66 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UV (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff8265b13b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 637.5560 - mse: 637.5560WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff821707320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 706ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "learning rate is 1\n",
      "Model: \"model_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_68 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UV (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff8244aa320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 637.5560 - mse: 637.5560WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff827e69a70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 504ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n"
     ]
    }
   ],
   "source": [
    "lrs = [0.01, 0.1, 0.5, 1]\n",
    "for lr in lrs:\n",
    "    print(\"learning rate is \"+ str(lr))\n",
    "    model_pipeline(train, test, 0, 'softmax', 3, len(train), 'UV', lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate is 0.01\n",
      "Model: \"model_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_70 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff82470a440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 24.2310 - mse: 24.2310WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff826c59170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 501ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "learning rate is 0.1\n",
      "Model: \"model_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_72 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff820e8f560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 24.2310 - mse: 24.2310WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff822a22320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 766ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "learning rate is 0.5\n",
      "Model: \"model_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_74 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff8273c8680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 24.2310 - mse: 24.2310WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff82fdde7a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 836ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "learning rate is 1\n",
      "Model: \"model_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_76 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff824eb03b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 24.2310 - mse: 24.2310WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff82ad08c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 733ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n"
     ]
    }
   ],
   "source": [
    "lrs = [0.01, 0.1, 0.5, 1]\n",
    "for lr in lrs:\n",
    "    print(\"learning rate is \"+ str(lr))\n",
    "    model_pipeline(train, test, 0, 'softmax', 3, len(train), 'UG', lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tunning the learning rate doesn't help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epoch: 3\n",
      "Model: \"model_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_78 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UV (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff820e8f8c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 637.5560 - mse: 637.5560WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff824ab9cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 696ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "\n",
      "\n",
      "Number of epoch: 5\n",
      "Model: \"model_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_80 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UV (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff8244aacb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 637.5560 - mse: 637.5560WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff8217070e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 667ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "\n",
      "\n",
      "Number of epoch: 7\n",
      "Model: \"model_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_82 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UV (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/7\n",
      "1/1 [==============================] - 1s 693ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 2/7\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 3/7\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 4/7\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 5/7\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 6/7\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 7/7\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "\n",
      "\n",
      "Number of epoch: 9\n",
      "Model: \"model_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_84 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UV (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 749ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 2/9\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 3/9\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 4/9\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 5/9\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 6/9\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 7/9\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 8/9\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "Epoch 9/9\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 637.5560 - mse: 637.5560 - val_loss: 632.4135 - val_mse: 632.4135\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoches = [3, 5, 7, 9]\n",
    "for epoch in epoches:\n",
    "    print(\"Number of epoch: \"+str(epoch))\n",
    "    model_pipeline(train, test, 1, 'softmax', epoch, len(train), 'UV', 0.1)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epoch: 79619\n",
      "Model: \"model_42\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_86 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "1/1 [==============================] - 1s 728ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "\n",
      "\n",
      "Number of epoch: 79619\n",
      "Model: \"model_43\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_88 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 1s 680ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "\n",
      "\n",
      "Number of epoch: 79619\n",
      "Model: \"model_44\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_90 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/7\n",
      "1/1 [==============================] - 0s 499ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 2/7\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 3/7\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 4/7\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 5/7\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 6/7\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 7/7\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "\n",
      "\n",
      "Number of epoch: 79619\n",
      "Model: \"model_45\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_92 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG (Dense)                   (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/9\n",
      "1/1 [==============================] - 0s 487ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 2/9\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 3/9\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 4/9\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 5/9\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 6/9\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 7/9\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 8/9\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "Epoch 9/9\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 24.2310 - mse: 24.2310 - val_loss: 23.8158 - val_mse: 23.8158\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoches = [3, 5, 7, 9]\n",
    "for epoch in epoches:\n",
    "    print(\"Number of epoch: \"+str(batch))\n",
    "    model_pipeline(train, test, 1, 'softmax', epoch, len(train), 'UG', 0.1)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of epoch doesn't help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For UV model\n",
    "* nums doesn't help. \n",
    "* sigmoid and softmax are roughly the same.\n",
    "\n",
    "For UG model\n",
    "* nums doesn't help, even starts to overfitting.\n",
    "* softmax is the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-output model\n",
    "Step 1. trans or not -- it doesn't matter -- no trans\n",
    "\n",
    "Step 2. batch_size -- it doesn't matter -- batch_size = len(trains)\n",
    "\n",
    "Step 3. which act_func and number of dense layer and epoch\n",
    "* Best act_func is sigmoid\n",
    "* Number of dense layer doesn't help to increase the accuracy measured by mse.\n",
    "* The larger the number of epoch, the better the model. But when epoch > 7, the influcence is insignificant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. trans or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, train_size = 0.9, random_state=42)\n",
    "train_trans, test_trans = train_test_split(data_trans, train_size = 0.9, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_model(inputs, num, act):\n",
    "    inputs = tf.keras.Input(shape=(7))\n",
    "    x = tf.keras.layers.Dense(7, activation = act)(inputs)\n",
    "    if num >= 1:\n",
    "        for _ in range(num-1):\n",
    "            x = tf.keras.layers.Dense(7, activation = act)(x)\n",
    "            \n",
    "    outputs = tf.keras.layers.Dense(2, name = 'UG_and_UV', activation = act)(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_and_compile_model(num, act):\n",
    "    '''define and compile the model\n",
    "    num: int -- number of dense layer in the model - 1\n",
    "    act: str -- activation function\n",
    "    '''\n",
    "    inputs = tf.keras.Input(shape = (7))\n",
    "    model = final_model(inputs, num, act)\n",
    "    \n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "    model.compile(optimizer = opt, loss = 'mse', metrics = 'mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(train, test, num, act, epoch, batch):\n",
    "    model = define_and_compile_model(num, act)\n",
    "    model.summary()\n",
    "    history = model.fit(train[features_name], train[['UG', 'UV']], epochs = epoch, batch_size = batch, validation_data = (test[features_name], test[['UG','UV']]))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_46\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_94 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG_and_UV (Dense)            (None, 2)                 16        \n",
      "=================================================================\n",
      "Total params: 72\n",
      "Trainable params: 72\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "1/1 [==============================] - 0s 492ms/step - loss: 358.6691 - mse: 358.6691 - val_loss: 355.7348 - val_mse: 355.7348\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 358.6691 - mse: 358.6691 - val_loss: 355.7348 - val_mse: 355.7348\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 358.6691 - mse: 358.6691 - val_loss: 355.7348 - val_mse: 355.7348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x7ff8217295d0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipeline(train_trans, test_trans, 0, 'relu', 3, len(train_trans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_47\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_96 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG_and_UV (Dense)            (None, 2)                 16        \n",
      "=================================================================\n",
      "Total params: 72\n",
      "Trainable params: 72\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "1/1 [==============================] - 0s 467ms/step - loss: 4337079.0000 - mse: 4337079.0000 - val_loss: 355.7348 - val_mse: 355.7348\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 358.6691 - mse: 358.6691 - val_loss: 355.7348 - val_mse: 355.7348\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 358.6691 - mse: 358.6691 - val_loss: 355.7348 - val_mse: 355.7348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x7ff82315f2d0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipeline(train, test, 0, 'relu', 3, len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_48\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_98 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG_and_UV (Dense)            (None, 2)                 16        \n",
      "=================================================================\n",
      "Total params: 72\n",
      "Trainable params: 72\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "797/797 [==============================] - 1s 1ms/step - loss: 357.5520 - mse: 357.5520 - val_loss: 355.7349 - val_mse: 355.7349\n",
      "Epoch 2/3\n",
      "797/797 [==============================] - 1s 1ms/step - loss: 358.6860 - mse: 358.6860 - val_loss: 355.7349 - val_mse: 355.7349\n",
      "Epoch 3/3\n",
      "797/797 [==============================] - 1s 1ms/step - loss: 359.5433 - mse: 359.5433 - val_loss: 355.7349 - val_mse: 355.7349\n",
      "Model: \"model_49\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_100 (InputLayer)       [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG_and_UV (Dense)            (None, 2)                 16        \n",
      "=================================================================\n",
      "Total params: 72\n",
      "Trainable params: 72\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "80/80 [==============================] - 1s 4ms/step - loss: 77981.2243 - mse: 77981.2243 - val_loss: 355.7348 - val_mse: 355.7348\n",
      "Epoch 2/3\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 358.4787 - mse: 358.4787 - val_loss: 355.7348 - val_mse: 355.7348\n",
      "Epoch 3/3\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 357.9487 - mse: 357.9487 - val_loss: 355.7348 - val_mse: 355.7348\n",
      "Model: \"model_50\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_102 (InputLayer)       [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG_and_UV (Dense)            (None, 2)                 16        \n",
      "=================================================================\n",
      "Total params: 72\n",
      "Trainable params: 72\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "8/8 [==============================] - 1s 44ms/step - loss: 249852.4488 - mse: 249852.4488 - val_loss: 355.7348 - val_mse: 355.7348\n",
      "Epoch 2/3\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 358.5853 - mse: 358.5853 - val_loss: 355.7348 - val_mse: 355.7348\n",
      "Epoch 3/3\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 358.6463 - mse: 358.6463 - val_loss: 355.7348 - val_mse: 355.7348\n",
      "Model: \"model_51\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_104 (InputLayer)       [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG_and_UV (Dense)            (None, 2)                 16        \n",
      "=================================================================\n",
      "Total params: 72\n",
      "Trainable params: 72\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "1/1 [==============================] - 1s 507ms/step - loss: 447591.0000 - mse: 447591.0000 - val_loss: 355.7348 - val_mse: 355.7348\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 358.6691 - mse: 358.6691 - val_loss: 355.7348 - val_mse: 355.7348\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 358.6691 - mse: 358.6691 - val_loss: 355.7348 - val_mse: 355.7348\n"
     ]
    }
   ],
   "source": [
    "batches = [100, 1000, 10000, len(train)]\n",
    "for batch in batches:\n",
    "    model_pipeline(train, test, 0, 'relu', 3, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. act and number of dense_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation func is relu\n",
      "Number of dense layer is 1\n",
      "Model: \"model_52\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_106 (InputLayer)       [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG_and_UV (Dense)            (None, 2)                 16        \n",
      "=================================================================\n",
      "Total params: 72\n",
      "Trainable params: 72\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "1/1 [==============================] - 0s 463ms/step - loss: 358.6691 - mse: 358.6691 - val_loss: 355.7348 - val_mse: 355.7348\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 358.6691 - mse: 358.6691 - val_loss: 355.7348 - val_mse: 355.7348\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 358.6691 - mse: 358.6691 - val_loss: 355.7348 - val_mse: 355.7348\n",
      "Activation func is relu\n",
      "Number of dense layer is 2\n",
      "Model: \"model_53\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_108 (InputLayer)       [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG_and_UV (Dense)            (None, 2)                 16        \n",
      "=================================================================\n",
      "Total params: 128\n",
      "Trainable params: 128\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "1/1 [==============================] - ETA: 0s - loss: 49140.4062 - mse: 49140.4062WARNING:tensorflow:5 out of the last 37 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff820e6f0e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 504ms/step - loss: 49140.4062 - mse: 49140.4062 - val_loss: 355.7348 - val_mse: 355.7348\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 358.6691 - mse: 358.6691 - val_loss: 355.7348 - val_mse: 355.7348\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 358.6691 - mse: 358.6691 - val_loss: 355.7348 - val_mse: 355.7348\n",
      "Activation func is relu\n",
      "Number of dense layer is 3\n",
      "Model: \"model_54\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_110 (InputLayer)       [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG_and_UV (Dense)            (None, 2)                 16        \n",
      "=================================================================\n",
      "Total params: 184\n",
      "Trainable params: 184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 34 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff821235050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 358.6691 - mse: 358.6691WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff826c59680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 592ms/step - loss: 358.6691 - mse: 358.6691 - val_loss: 355.7348 - val_mse: 355.7348\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 358.6691 - mse: 358.6691 - val_loss: 355.7348 - val_mse: 355.7348\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 358.6691 - mse: 358.6691 - val_loss: 355.7348 - val_mse: 355.7348\n",
      "Activation func is sigmoid\n",
      "Number of dense layer is 1\n",
      "Model: \"model_55\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_112 (InputLayer)       [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG_and_UV (Dense)            (None, 2)                 16        \n",
      "=================================================================\n",
      "Total params: 72\n",
      "Trainable params: 72\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff8247537a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 335.4041 - mse: 335.4041WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff81ea565f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 466ms/step - loss: 335.4041 - mse: 335.4041 - val_loss: 329.7106 - val_mse: 329.7106\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 332.5062 - mse: 332.5062 - val_loss: 329.1884 - val_mse: 329.1884\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 331.9786 - mse: 331.9786 - val_loss: 328.9051 - val_mse: 328.9051\n",
      "Activation func is sigmoid\n",
      "Number of dense layer is 2\n",
      "Model: \"model_56\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_114 (InputLayer)       [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG_and_UV (Dense)            (None, 2)                 16        \n",
      "=================================================================\n",
      "Total params: 128\n",
      "Trainable params: 128\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff822a503b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 346.9896 - mse: 346.9896WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff81bc39e60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 608ms/step - loss: 346.9896 - mse: 346.9896 - val_loss: 334.8689 - val_mse: 334.8689\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 337.7014 - mse: 337.7014 - val_loss: 332.2128 - val_mse: 332.2128\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 335.0318 - mse: 335.0318 - val_loss: 331.3213 - val_mse: 331.3213\n",
      "Activation func is sigmoid\n",
      "Number of dense layer is 3\n",
      "Model: \"model_57\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_116 (InputLayer)       [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG_and_UV (Dense)            (None, 2)                 16        \n",
      "=================================================================\n",
      "Total params: 184\n",
      "Trainable params: 184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff822a22440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 341.9257 - mse: 341.9257WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff8247f5d40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 660ms/step - loss: 341.9257 - mse: 341.9257 - val_loss: 332.9295 - val_mse: 332.9295\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 335.7465 - mse: 335.7465 - val_loss: 331.4170 - val_mse: 331.4170\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 334.2257 - mse: 334.2257 - val_loss: 330.6955 - val_mse: 330.6955\n",
      "Activation func is softmax\n",
      "Number of dense layer is 1\n",
      "Model: \"model_58\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_118 (InputLayer)       [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG_and_UV (Dense)            (None, 2)                 16        \n",
      "=================================================================\n",
      "Total params: 72\n",
      "Trainable params: 72\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff8253dc440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 347.1653 - mse: 347.1653WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff82654db90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 526ms/step - loss: 347.1653 - mse: 347.1653 - val_loss: 337.0009 - val_mse: 337.0009\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 339.8228 - mse: 339.8228 - val_loss: 333.4941 - val_mse: 333.4941\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 336.3230 - mse: 336.3230 - val_loss: 332.8927 - val_mse: 332.8927\n",
      "Activation func is softmax\n",
      "Number of dense layer is 2\n",
      "Model: \"model_59\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_120 (InputLayer)       [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG_and_UV (Dense)            (None, 2)                 16        \n",
      "=================================================================\n",
      "Total params: 128\n",
      "Trainable params: 128\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff8273a1f80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 345.8055 - mse: 345.8055WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff8247ad9e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 529ms/step - loss: 345.8055 - mse: 345.8055 - val_loss: 337.3968 - val_mse: 337.3968\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 340.2390 - mse: 340.2390 - val_loss: 334.4907 - val_mse: 334.4907\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 337.3231 - mse: 337.3231 - val_loss: 333.5429 - val_mse: 333.5429\n",
      "Activation func is softmax\n",
      "Number of dense layer is 3\n",
      "Model: \"model_60\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_122 (InputLayer)       [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG_and_UV (Dense)            (None, 2)                 16        \n",
      "=================================================================\n",
      "Total params: 184\n",
      "Trainable params: 184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff8247d6b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 343.5413 - mse: 343.5413WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff8273a1290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 580ms/step - loss: 343.5413 - mse: 343.5413 - val_loss: 335.8665 - val_mse: 335.8665\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 338.7035 - mse: 338.7035 - val_loss: 334.0497 - val_mse: 334.0497\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 336.8805 - mse: 336.8805 - val_loss: 333.3745 - val_mse: 333.3745\n"
     ]
    }
   ],
   "source": [
    "act_funcs = ['relu', 'sigmoid','softmax']\n",
    "nums = [1, 2, 3]\n",
    "for act in act_funcs:\n",
    "    for num in nums:\n",
    "        print(\"Activation func is \"+act)\n",
    "        print(\"Number of dense layer is \"+str(num))\n",
    "        model_pipeline(train, test, num, act, 3, len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Act_funcs should be sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation func is sigmoid\n",
      "Number of dense layer is 1\n",
      "Model: \"model_61\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_124 (InputLayer)       [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG_and_UV (Dense)            (None, 2)                 16        \n",
      "=================================================================\n",
      "Total params: 72\n",
      "Trainable params: 72\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff823b898c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 350.5962 - mse: 350.5962WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff824979b90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 678ms/step - loss: 350.5962 - mse: 350.5962 - val_loss: 335.1047 - val_mse: 335.1047\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 337.9391 - mse: 337.9391 - val_loss: 330.9489 - val_mse: 330.9489\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 333.7539 - mse: 333.7539 - val_loss: 330.1823 - val_mse: 330.1823\n",
      "\n",
      "\n",
      "Activation func is sigmoid\n",
      "Number of dense layer is 5\n",
      "Model: \"model_62\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_126 (InputLayer)       [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG_and_UV (Dense)            (None, 2)                 16        \n",
      "=================================================================\n",
      "Total params: 296\n",
      "Trainable params: 296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff82ab313b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 347.4076 - mse: 347.4076WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff823b897a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 912ms/step - loss: 347.4076 - mse: 347.4076 - val_loss: 336.0232 - val_mse: 336.0232\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 338.8496 - mse: 338.8496 - val_loss: 331.6003 - val_mse: 331.6003\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 334.4045 - mse: 334.4045 - val_loss: 330.4161 - val_mse: 330.4161\n",
      "\n",
      "\n",
      "Activation func is sigmoid\n",
      "Number of dense layer is 10\n",
      "Model: \"model_63\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_128 (InputLayer)       [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG_and_UV (Dense)            (None, 2)                 16        \n",
      "=================================================================\n",
      "Total params: 576\n",
      "Trainable params: 576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff82a8ba560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - ETA: 0s - loss: 351.0993 - mse: 351.0993WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff81e38de60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 1s/step - loss: 351.0993 - mse: 351.0993 - val_loss: 340.3679 - val_mse: 340.3679\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 343.2258 - mse: 343.2258 - val_loss: 334.0103 - val_mse: 334.0103\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 336.8374 - mse: 336.8374 - val_loss: 332.4479 - val_mse: 332.4479\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "act_funcs = ['sigmoid']\n",
    "nums = [1, 5, 10]\n",
    "for act in act_funcs:\n",
    "    for num in nums:\n",
    "        print(\"Activation func is \"+act)\n",
    "        print(\"Number of dense layer is \"+str(num))\n",
    "        model_pipeline(train, test, num, act, 3, len(train))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding more dense layer doesn't work. set dense layer to be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epoch: 79619\n",
      "Model: \"model_64\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_130 (InputLayer)       [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG_and_UV (Dense)            (None, 2)                 16        \n",
      "=================================================================\n",
      "Total params: 72\n",
      "Trainable params: 72\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff80be3b050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 339.7534 - mse: 339.7534WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff8249967a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 1s/step - loss: 339.7534 - mse: 339.7534 - val_loss: 329.5072 - val_mse: 329.5072\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 332.2996 - mse: 332.2996 - val_loss: 329.1243 - val_mse: 329.1243\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 331.9131 - mse: 331.9131 - val_loss: 328.8965 - val_mse: 328.8965\n",
      "\n",
      "\n",
      "Number of epoch: 79619\n",
      "Model: \"model_65\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_132 (InputLayer)       [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG_and_UV (Dense)            (None, 2)                 16        \n",
      "=================================================================\n",
      "Total params: 72\n",
      "Trainable params: 72\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff821235c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 340.0731 - mse: 340.0731WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff823bbc200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 681ms/step - loss: 340.0731 - mse: 340.0731 - val_loss: 329.8693 - val_mse: 329.8693\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 332.6680 - mse: 332.6680 - val_loss: 329.2826 - val_mse: 329.2826\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 332.0746 - mse: 332.0746 - val_loss: 328.9344 - val_mse: 328.9344\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 331.7224 - mse: 331.7224 - val_loss: 328.7372 - val_mse: 328.7372\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 331.5230 - mse: 331.5230 - val_loss: 328.6161 - val_mse: 328.6161\n",
      "\n",
      "\n",
      "Number of epoch: 79619\n",
      "Model: \"model_66\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_134 (InputLayer)       [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG_and_UV (Dense)            (None, 2)                 16        \n",
      "=================================================================\n",
      "Total params: 72\n",
      "Trainable params: 72\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/7\n",
      "1/1 [==============================] - 1s 597ms/step - loss: 338.9704 - mse: 338.9704 - val_loss: 329.6529 - val_mse: 329.6529\n",
      "Epoch 2/7\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 332.4433 - mse: 332.4433 - val_loss: 329.1328 - val_mse: 329.1328\n",
      "Epoch 3/7\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 331.9192 - mse: 331.9192 - val_loss: 328.9031 - val_mse: 328.9031\n",
      "Epoch 4/7\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 331.6886 - mse: 331.6886 - val_loss: 328.7632 - val_mse: 328.7632\n",
      "Epoch 5/7\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 331.5477 - mse: 331.5477 - val_loss: 328.6658 - val_mse: 328.6658\n",
      "Epoch 6/7\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 331.4497 - mse: 331.4497 - val_loss: 328.5934 - val_mse: 328.5934\n",
      "Epoch 7/7\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 331.3763 - mse: 331.3763 - val_loss: 328.5323 - val_mse: 328.5323\n",
      "\n",
      "\n",
      "Number of epoch: 79619\n",
      "Model: \"model_67\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_136 (InputLayer)       [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "UG_and_UV (Dense)            (None, 2)                 16        \n",
      "=================================================================\n",
      "Total params: 72\n",
      "Trainable params: 72\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 473ms/step - loss: 343.4080 - mse: 343.4080 - val_loss: 329.7747 - val_mse: 329.7747\n",
      "Epoch 2/9\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 332.5660 - mse: 332.5660 - val_loss: 329.2251 - val_mse: 329.2251\n",
      "Epoch 3/9\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 332.0129 - mse: 332.0129 - val_loss: 328.9711 - val_mse: 328.9711\n",
      "Epoch 4/9\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 331.7572 - mse: 331.7572 - val_loss: 328.8163 - val_mse: 328.8163\n",
      "Epoch 5/9\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 331.6011 - mse: 331.6011 - val_loss: 328.7102 - val_mse: 328.7102\n",
      "Epoch 6/9\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 331.4943 - mse: 331.4943 - val_loss: 328.6324 - val_mse: 328.6324\n",
      "Epoch 7/9\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 331.4159 - mse: 331.4159 - val_loss: 328.5728 - val_mse: 328.5728\n",
      "Epoch 8/9\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 331.3557 - mse: 331.3557 - val_loss: 328.5255 - val_mse: 328.5255\n",
      "Epoch 9/9\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 331.3081 - mse: 331.3081 - val_loss: 328.4872 - val_mse: 328.4872\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoches = [3, 5, 7, 9]\n",
    "for epoch in epoches:\n",
    "    print(\"Number of epoch: \"+str(batch))\n",
    "    model_pipeline(train, test, 1, 'sigmoid', epoch, len(train))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The larger the epoch, the better the model. But when epoch is larger than 7, the descent is not significant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
